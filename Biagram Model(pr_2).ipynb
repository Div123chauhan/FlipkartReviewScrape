{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Biagram Model (pr_2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuGOH_LvbY-z",
        "outputId": "e8641187-505c-4e1d-bddc-d763e35202f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters') # one time execution\n",
        "nltk.download('punkt')  # one time execution\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "import math\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DL-Y571b_hF",
        "outputId": "ae0332bd-8aa6-4b4f-dace-702e3b0c2767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# see the data\n",
        "reuters.raw()[:260]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reaching economic\\n  damage, businessmen and officials said.\\n      T\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5nWs8VqcNqb",
        "outputId": "cbca67b2-6965-4f0b-cce5-206efc58377e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# get sentences\n",
        "dataset_sents = reuters.sents()\n",
        "# print first sentence\n",
        "dataset_sents[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['They',\n",
              " 'told',\n",
              " 'Reuter',\n",
              " 'correspondents',\n",
              " 'in',\n",
              " 'Asian',\n",
              " 'capitals',\n",
              " 'a',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'Move',\n",
              " 'against',\n",
              " 'Japan',\n",
              " 'might',\n",
              " 'boost',\n",
              " 'protectionist',\n",
              " 'sentiment',\n",
              " 'in',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'And',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'curbs',\n",
              " 'on',\n",
              " 'American',\n",
              " 'imports',\n",
              " 'of',\n",
              " 'their',\n",
              " 'products',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGLHyiIDcU8O",
        "outputId": "1edded9e-ec1a-4abc-caaa-694224ac9ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of sentences\n",
        "len(dataset_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z96vh_URceuL",
        "outputId": "18f4a0ea-875b-4f5f-f60e-6e7b335ce4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# get list of words\n",
        "dataset_words = reuters.words()\n",
        "# print first word\n",
        "dataset_words[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN',\n",
              " 'EXPORTERS',\n",
              " 'FEAR',\n",
              " 'DAMAGE',\n",
              " 'FROM',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.-',\n",
              " 'JAPAN',\n",
              " 'RIFT',\n",
              " 'Mounting',\n",
              " 'trade',\n",
              " 'friction',\n",
              " 'between',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB5_dnQ0cixt",
        "outputId": "7cf890be-67e8-4fda-da4b-71c53cfdcd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# total number of words\n",
        "len(dataset_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1720901"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ko3VAJQcoam",
        "outputId": "65bb558a-eb99-4095-9981-69055e60dbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# size of vocabulary\n",
        "len(set(dataset_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7wZCKhrc005"
      },
      "source": [
        "data_sents = dataset_sents[:40000]\n",
        "data_sents_test = dataset_sents[40000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6EvFjtNc6d9",
        "outputId": "a1bb8109-4e6f-47e4-a7af-a83b494bc67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of words in train data\n",
        "num_words = 0\n",
        "for sentence in data_sents:\n",
        "  num_words += len(sentence)\n",
        "num_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1262448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kafHYW2UdA7z"
      },
      "source": [
        "# create two lists containing words\n",
        "data_words_train = dataset_words[:num_words]\n",
        "data_words_test = dataset_words[num_words:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwXkTPMidYkx"
      },
      "source": [
        "def createBigram(data):\n",
        "\tlistOfBigrams = []\n",
        "\tbigramCounts = {}\n",
        "\tunigramCounts = {}\n",
        "\n",
        "\tfor i in range(len(data)):\n",
        "\t\tif i < len(data) - 1:\n",
        "\n",
        "\t\t\tlistOfBigrams.append((data[i], data[i + 1]))\n",
        "\n",
        "\t\t\tif (data[i], data[i+1]) in bigramCounts:\n",
        "\t\t\t\tbigramCounts[(data[i], data[i + 1])] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tbigramCounts[(data[i], data[i + 1])] = 1\n",
        "\n",
        "\t\tif data[i] in unigramCounts:\n",
        "\t\t\tunigramCounts[data[i]] += 1\n",
        "\t\telse:\n",
        "\t\t\tunigramCounts[data[i]] = 1\n",
        "\n",
        "\treturn listOfBigrams, unigramCounts, bigramCounts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cT-AkLsda-a"
      },
      "source": [
        "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
        "\n",
        "\tlistOfProb = {}\n",
        "\tfor bigram in listOfBigrams:\n",
        "\t\tword1 = bigram[0]\n",
        "\t\tword2 = bigram[1]\n",
        "\t\t\n",
        "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
        "\n",
        "\tfile = open('bigramProb.txt', 'w')\n",
        "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
        "\n",
        "\tfor bigrams in listOfBigrams:\n",
        "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams]) + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
        "\tfile.close()\n",
        "\n",
        "\treturn listOfProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIr3RNFDdh5B"
      },
      "source": [
        "def bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts):\n",
        "\n",
        "\tlistOfProb = {}\n",
        "\tcStar = {}\n",
        "\n",
        "\n",
        "\tfor bigram in listOfBigrams:\n",
        "\t\tword1 = bigram[0]\n",
        "\t\tword2 = bigram[1]\n",
        "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram) + 1)/(unigramCounts.get(word1) + len(unigramCounts))\n",
        "\t\tcStar[bigram] = (bigramCounts[bigram] + 1) * unigramCounts[word1] / (unigramCounts[word1] + len(unigramCounts))\n",
        "\n",
        "\tfile = open('addOneSmoothing.txt', 'w')\n",
        "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
        "\n",
        "\tfor bigrams in listOfBigrams:\n",
        "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams])\n",
        "\t\t\t\t   + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
        "\n",
        "\tfile.close()\n",
        "\n",
        "\treturn listOfProb, cStar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-t5gz4OdomQ"
      },
      "source": [
        "# Main Program\n",
        "\n",
        "# Create a list of bigrams and get frequencies of unigrams and bigrams\n",
        "listOfBigrams, unigramCounts, bigramCounts = createBigram(data_words_train)\n",
        "\n",
        "# Calculate bigram probabilities\n",
        "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
        "\n",
        "# Apply Add-1 Smoothing and calculate probabilities and get reconstructed count of bigrams\n",
        "bigramAddOne, addOneCstar = bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGiCITTud8bS",
        "outputId": "8cabbfeb-cc91-4bd1-d9d4-c88b1cf1ab00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = 'we must be very careful'\n",
        "\n",
        "inputList = [] # list to store bigrams\n",
        "\n",
        "for i in range(len(input.split())-1):\n",
        "  inputList.append((input.split()[i], input.split()[i+1]))\n",
        "inputList"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws42d88ufpe4",
        "outputId": "e02ef18b-45ec-40ba-c7c3-f7504ca89942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Open a file to write output\n",
        "output1 = open('bigramProb-OUTPUT.txt', 'w')\n",
        "\n",
        "# initial probability of a sentence\n",
        "outputProb1 = 1\n",
        "\n",
        "output1.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  # if bigram is present in the model, get updated probability\n",
        "  if inputList[i] in bigramProb:\n",
        "    # write bigram, its count and probability to the file\n",
        "    output1.write(str(inputList[i]) + '\\t\\t' + str(bigramCounts[inputList[i]]) + '\\t\\t' + str(bigramProb[inputList[i]]) + '\\n')\n",
        "    # multiply with probability of a current bigram\n",
        "    outputProb1 *= bigramProb[inputList[i]]\n",
        "\n",
        "  # if bigram is not present in the model, sentence probability is zero\n",
        "  else:\n",
        "    output1.write(str(inputList[i]) + '\\t\\t\\t' + str(0) + '\\t\\t\\t' + str(0) + '\\n')\n",
        "    outputProb1 *= 0\n",
        "\n",
        "output1.write('\\n' + 'Probablility = ' + str(outputProb1))\n",
        "outputProb1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8076159793430567e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXuHjwh8eqaZ",
        "outputId": "14fe97a7-ca8c-469d-b674-fdb5997af2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Open a file to write output\n",
        "output2 = open('addOneSmoothing-OUTPUT.txt', 'w')\n",
        "\n",
        "# initial probability of a sentence\n",
        "outputProb2 = 1\n",
        "\n",
        "output2.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  # if bigram is present in the model, get updated probability\n",
        "  if inputList[i] in bigramAddOne:\n",
        "    # Update probability of the sentence\n",
        "    outputProb2 *= bigramAddOne[inputList[i]]\n",
        "\n",
        "    output2.write(str(inputList[i]) + '\\t\\t' + str(addOneCstar[inputList[i]]) + '\\t\\t' + str(bigramAddOne[inputList[i]]) + '\\n')\n",
        "\n",
        "  # if bigram is not present in the model, use unigram counts to get estimated probability\n",
        "  else:\n",
        "    # if first word in a bigram is not present in unigrams, add with with count 1\n",
        "    if inputList[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList[i][0]] = 1\n",
        "    \n",
        "    # calculate probability of that word\n",
        "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "\n",
        "    # # reconstructed count for the bigram\n",
        "    addOneCStar = 1 * unigramCounts[inputList[i][0]] / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "    \n",
        "    # Update probability of the sentence\n",
        "    outputProb2 *= prob\n",
        "\n",
        "    output2.write(str(inputList[i]) + '\\t' + str(addOneCStar) + '\\t' + str(prob) + '\\n')\n",
        "\n",
        "output2.write('\\n' + 'Probablility = ' + str(outputProb2))\n",
        "outputProb2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2254340301928457e-14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXcPYzG7f3kR",
        "outputId": "1ebd6c31-ab25-4f79-8b6f-32638ab9f9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# input sentence\n",
        "print(input)\n",
        "\n",
        "# list of bigrams\n",
        "print(inputList)\n",
        "\n",
        "# probability given by simple bigram model\n",
        "print ('Bigram Model: ', outputProb1)\n",
        "\n",
        "# probability given by bigram model with add-1 smoothing\n",
        "print ('Add One: ', outputProb2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we must be very careful\n",
            "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]\n",
            "Bigram Model:  2.8076159793430567e-07\n",
            "Add One:  4.2254340301928457e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brROTmR_u6em"
      },
      "source": [
        "def sentence_prob_with_next_word(next_word):\n",
        "  outputProb = 1\n",
        "  new_bigram = (input.split()[-1], next_word)\n",
        "  if new_bigram in bigramAddOne:\n",
        "    outputProb *= bigramAddOne[new_bigram]\n",
        "  else:\n",
        "    if new_bigram[0] not in unigramCounts:\n",
        "      unigramCounts[new_bigram[0]] = 1\n",
        "    prob = (1) / (unigramCounts[new_bigram[0]] + len(unigramCounts))\n",
        "    outputProb *= prob\n",
        "  return outputProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGgWz-3GvCRd"
      },
      "source": [
        "input = 'the investors are'\n",
        "possible_words = ['cheated', 'happy', 'smart', 'afraid']\n",
        "\n",
        "inputList = []\n",
        "outputProb = 1\n",
        "\n",
        "for i in range(len(input.split())-1):\n",
        "  inputList.append((input.split()[i], input.split()[i+1]))\n",
        "\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  if inputList[i] in bigramAddOne:\n",
        "    outputProb *= bigramAddOne[inputList[i]]\n",
        "  else:\n",
        "    if inputList[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList[i][0]] = 1\n",
        "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "    outputProb *= prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9IWawGKvIJc",
        "outputId": "047b4f3e-684d-447b-c692-4021755027a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_prob = 0\n",
        "index_of_next_word = -1\n",
        "for i, word in enumerate(possible_words):\n",
        "  final_prob = outputProb * sentence_prob_with_next_word(word)\n",
        "  if final_prob > max_prob:\n",
        "    max_prob = final_prob\n",
        "    index_of_next_word = i\n",
        "\n",
        "print('Next Word:', possible_words[index_of_next_word])\n",
        "print('Output Sentece:', input, possible_words[index_of_next_word])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next Word: happy\n",
            "Output Sentece: the investors are happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dbAAQPPvVht",
        "outputId": "49f9f7e6-552a-47af-b421-23642f5ea79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input1 = 'the market is very happy these days'\n",
        "input2 = 'market is the happy these very days'\n",
        "\n",
        "\n",
        "inputList1 = []\n",
        "inputList2 = []\n",
        "\n",
        "\n",
        "outputProb1 = 1\n",
        "outputProb2 = 1\n",
        "\n",
        "\n",
        "for i in range(len(input1.split())-1):\n",
        "  inputList1.append((input1.split()[i], input1.split()[i+1]))\n",
        "\n",
        "for i in range(len(input2.split())-1):\n",
        "  inputList2.append((input2.split()[i], input2.split()[i+1]))\n",
        "\n",
        "\n",
        "for i in range(len(inputList1)):\n",
        "  if inputList1[i] in bigramAddOne:\n",
        "    outputProb1 *= bigramAddOne[inputList1[i]]\n",
        "  else:\n",
        "    if inputList1[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList1[i][0]] = 1\n",
        "    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n",
        "    outputProb1 *= prob1\n",
        "\n",
        "\n",
        "for i in range(len(inputList2)):\n",
        "  if inputList2[i] in bigramAddOne:\n",
        "    outputProb2 *= bigramAddOne[inputList2[i]]\n",
        "  else:\n",
        "    if inputList2[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList2[i][0]] = 1\n",
        "    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n",
        "    outputProb2 *= prob2\n",
        "\n",
        "print (input1, ':', outputProb1)\n",
        "print (input2, ':', outputProb2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the market is very happy these days : 3.0787233025784113e-22\n",
            "market is the happy these very days : 2.5840603259051406e-24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLirIHevkYJ"
      },
      "source": [
        "def calculate_bigram_sentence_probability(input):\n",
        "\n",
        "  inputList = []\n",
        "  outputProb = 1\n",
        "\n",
        "  for i in range(len(input)-1):\n",
        "    inputList.append((input[i], input[i+1]))\n",
        "\n",
        "  for i in range(len(inputList)):\n",
        "    if inputList[i] in bigramAddOne:\n",
        "      outputProb *= bigramAddOne[inputList[i]]\n",
        "    else:\n",
        "      if inputList[i][0] not in unigramCounts:\n",
        "        unigramCounts[inputList[i][0]] = 1\n",
        "      prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "      outputProb *= prob\n",
        "\n",
        "  return outputProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvsDgGUxvmL1"
      },
      "source": [
        "def calculate_number_of_bigrams(sentences):\n",
        "        bigram_count = 0\n",
        "        for sentence in sentences:\n",
        "            # remove one for number of bigrams in sentence\n",
        "            bigram_count += len(sentence) - 1\n",
        "        return bigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGTgLl1Yvqr_"
      },
      "source": [
        "def calculate_bigram_perplexity(model, sentences):\n",
        "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
        "    bigram_sentence_probability_log_sum = 0\n",
        "    for sentence in sentences:\n",
        "        p = calculate_bigram_sentence_probability(sentence)\n",
        "        if p != 0.0:\n",
        "          a = math.log(p)\n",
        "        else:\n",
        "          a = 0\n",
        "        bigram_sentence_probability_log_sum -= a\n",
        "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Te_JrGwGFa",
        "outputId": "0f3f67a4-cc72-4ad4-926b-5da8ddf3a41c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"PERPLEXITY over Training Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents))\n",
        "print(\"PERPLEXITY over Test Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERPLEXITY over Training Data: 137.07939217258775\n",
            "PERPLEXITY over Test Data: 168.93416630740222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue31pueqwdFK",
        "outputId": "b9ab09df-311a-4695-87eb-b24c7241d0d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# initial word\n",
        "text = [\"there\"]\n",
        "\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = 0.0\n",
        "\n",
        "  for pair in bigramProb.keys():\n",
        "    if pair[0] == text[-1]:\n",
        "      accumulator += bigramProb[pair]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(pair[1])\n",
        "          break\n",
        "\n",
        "  if text[-1] == 'None':\n",
        "    sentence_finished = True\n",
        "  if len(text) > 20:\n",
        "    sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are liberalised 20 Record April 7 mln vs loss of a special meeting in December 1986 net loss 7 ,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}